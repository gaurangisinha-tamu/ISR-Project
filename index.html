<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Confluence" />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/twitter-teaser-tails.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Confluence.">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter-teaser-tails.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="VLMs,LLMs,GenAI,Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Confluence</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Confluence
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Anwesha Basu</a><sup></sup><sup></sup>,</span>
              <span class="author-block">
                <a target="_blank">Arunim Samudra</a><sup></sup><sup></sup>,</span>
              <span class="author-block">
                <a target="_blank">Gaurangi Sinha</a><sup></sup><sup></sup>,</span>
              <span class="author-block">
                <a target="_blank">Mihir Godbole</a><sup></sup></span>
              <!-- <span class="author-block">
                <a target="_blank">Tiffany Ling</a><sup>2</sup>,</span> -->

              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>Texas A&M University<br></span>
              <span class="eql-cntrb"><small><br><sup></sup>Advisor: James Caverlee</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- span class="link-block">
                  <a href="https://arxiv.org/pdf/2401.12425.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span -->

                <!-- Github link -->
                <!-- span class="link-block">
                  <a href="https://github.com/shubhamprshr27/NeglectedTailsVLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span-->

                <!-- ArXiv abstract Link -->
                <!-- span class="link-block">
                  <a href="https://arxiv.org/abs/2401.12425" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span -->
              </div>
            </div>
            <h1><strong style="color: red; font-size: x-large;">Information Storage and Retrieval Project 2024</strong></h1>
          </div>
        </div>
      </div>  
    </div>
  </section>
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">
              Vision-language models (VLMs) excel in zero-shot
recognition but their performance varies greatly across
different visual concepts. For example, although CLIP
achieves impressive accuracy on ImageNet (60-80%), its
performance drops below 10% for more than ten concepts
like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in VLMs’ large-scale datasets is challenging. We address this by using large language models
(LLMs) to count the number of pretraining texts that contain synonyms of these concepts. Our analysis confirms
that popular datasets, such as LAION, exhibit a long-tailed
concept distribution, yielding biased performance in VLMs.
We also find that downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and text-to-image models
(e.g., Stable Diffusion), often fail to recognize or generate
images of rare concepts identified by our method. To mitigate the imbalanced performance of zero-shot VLMs, we
propose REtrieval-Augmented Learning (REAL). First, instead of prompting VLMs using the original class names,
REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly
human-engineered and LLM-enriched prompts over nine
benchmark datasets. Second, REAL trains a linear classifier on a small yet balanced set of pretraining data retrieved using concept synonyms. REAL surpasses the previous zero-shot SOTA, using 400× less storage and 10,000×
less training time!
            </ul>  
              <!-- In this work, we make the first attempt to measure 
              the concept frequency in VLMs' pretraining data by analyzing pretraining 
              texts. We use off-the-shelf language models to help count relevant texts that 
              contain synonyms of the given concepts and resolve ambiguous cases. Our analysis 
              confirms that popular VLM datasets like LAION indeed exhibit a long-tailed 
              concept distribution, which strongly correlates with per-class accuracies. 
              Further, mainstream multimodal systems, including visual chatbots and 
              text-to-image models, struggle with the rare concepts identified by our method. 
              Next, to mitigate VLMs' imbalanced performance in zero-shot recognition, we 
              propose <strong>RE</strong>trieval-<strong>A</strong>ugmented <strong>L</strong>earning <strong>(REAL)</strong>. First, instead of prompting VLMs 
              using the original class names defined in a downstream task, REAL uses their 
              most frequent synonyms found in the pretraining texts. This already outperforms 
              human-engineered and LLM-generated prompts over nine benchmark datasets, likely 
              because VLMs have seen more images associated with the frequently used synonyms. 
              Second, REAL uses all the concept synonyms to retrieve a small, class-balanced 
              set of pretraining data to train a robust classifier. REAL surpasses the recent 
              retrieval-augmented solution REACT, using 400 times less storage and 10,000 
              times less training time! -->
            <!-- </p> -->
          
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Data Pre-Processing</h2>
          <!-- <h2 class="title is-4">Strong Correlation of Concept Frequency and Accuracy</h2> -->
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>-->
              <img src="static/images/Pre_Processing .png" alt="1" style="width: auto; height: auto; display: block; margin: 0 auto;"/> 
              <!--p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;" -->
                
              </p>
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <!-- section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Findings</h2>
          <h2 class="title is-4">VLMs show imbalanced performance due to a long-tailed concept distribution</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              <!-- <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/>
              <img src="static/images/imagenet_1k_freq.png" alt="1" style="width: 250px; height: auto; display: block; margin: 0 auto;"/> -->
              <!-- <img src="static/images/promptgpt.png" alt="Image illustrating ChatGPT interaction with VLMs"
                style="width: 850px; height: auto; display: block; margin: 0 auto;"> -->
                
    
    
                      
    
                          
    
                        </div>    
    
                      </div>
                    </div>
                  </div>
                </section>

              
            </div>
          </div>
        </div>
      </div>
    </div>
  </section >
  <!-- End Method Overview -->


  <!-- Text-Image generation -->
  
  <!-- End Prompt Inversion -->


  <!-- Prompt Inversion -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Two Tower Model</h2>
          <div class="content has-text-justified">
            <p>
              
            </p>
          </div>
          <h2 class="title is-4">Two Tower Model with CLIP</h2>
          <div class="content has-text-justified">
            <img src="static/images/Model Diagram.png" alt="Showcasing the efficiency of REAL-Prompt."
                style="height: auto; display: block; margin: 10px;">
              
              <div class="item">
                <!-- Your image here -->
                
              </div>
            <section class="hero is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item">
                      <!-- Image container -->
                      <div class="images-container">
                        <div class="image-with-subtitle">
                          <img src="static/images/TrainInference.png" alt="1" />
                          
                        </div>
                        
                      </div>
                      
                    </div>

                    


                    
                    </div>
                  </div>
                </div>
              
            </section>
            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->

<!-- Quantitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Inference</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/TrainInference.png" alt="Image showing DALL-E 3 output" />
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->

  <!-- Qualitative Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Data Stats - Source</h2>
          <div class="content">

            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                
              </p>
              <img src="static/images/Data_stats.png" alt="Image showing DALL-E 3 output" />
            </div>
            <div class="item">
              <!-- Your image here -->
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;"> 
                
              </p>
              <img src="static/images/data-stats-t.png" alt="Image showing DALL-E 3 output" />
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Qualitative Results -->

  <!-- Quantitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Inference</h2>
          <div class="content has-text-justified">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/TrainInference.png" alt="Image showing DALL-E 3 output" />
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->

  <!-- Quantitative Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Results</h2>
          <div class="content has-text-justified">
            <div class="item">

                <div class="query-container">
                  <p class="user-query">Source</p>
                </div>
            <div class="images-container">
    
                <div class="image-with-subtitle">
                  <img src="static/images/1.png" alt="1" style="height: 310px;"/>
                  <p class="subtitle-text">Precision@5</p>
                </div>
                <div class="image-with-subtitle">
                  <img src="static/images/2.png" alt="2" style="height: 310px;"/>
                  <p class="subtitle-text">Recall@5</p>
                </div>

              </div>
            </div>
    
            <div class="item">

                <div class="query-container">
                  <p class="user-query">Target</p>
                </div>
            <div class="images-container">
    
                <div class="image-with-subtitle">
                  <img src="static/images/3.png" alt="1" style="height: 310px;"/>
                  <p class="subtitle-text">Precision@5</p>
                </div>
                <div class="image-with-subtitle">
                  <img src="static/images/4.png" alt="2" style="height: 310px;"/>
                  <p class="subtitle-text">Recall@5</p>
                </div>

              </div>
            </div>

            </div>

            
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->


  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <h2 class="title">Copyright</h2>
      <pre><code>MAGA: Mihir, Anwesha, Gaurangi, Arunim
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
